{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Text Generator\n",
    "the goal of this project is to create a GRU - RNN to generate text.\n",
    "#use the two files - MLtrain.txt to train the model and MLValidate.txt to evaluate the model \n",
    "#run your code to generate text with starting triggers \"machine\", \"regression\", \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a text clean up process to generate vocabulary\n",
    "# <eos> is added to mark end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all helper functions and classes defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "    \n",
    "class Corpus(object):\n",
    "    def __init__(self, train, validate):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(train)\n",
    "        self.valid = self.tokenize(validate)\n",
    "    \n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
    "            tokens = 0\n",
    "            token = 0\n",
    "            ids = []\n",
    "            for line in f:\n",
    "                #line = ''.join([c for c in line if c in self.whitelist])\n",
    "                words = line.split()\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                    if word == '.':\n",
    "                        self.dictionary.add_word(\"<eos>\")\n",
    "                        ids.append(self.dictionary.word2idx[\"<eos>\"])\n",
    "                    token += 1\n",
    "        return torch.LongTensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, batch_size):\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(source, i, evaluation=False):\n",
    "    seq_len = min(bptt_size, len(source) - 1 - i)\n",
    "    #data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    with torch.no_grad():\n",
    "        data = Variable(source[i:i+seq_len])\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: load the two - train and validate files into the corpus object : 10 Pts\n",
    "\n",
    "Use sample.txt for training \n",
    "Use test.txt for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('sample.txt', 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: print the vocabulary size : 5 Pts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9431\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(corpus.dictionary)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: complete the code for RNNModel encoder decoder model - backprop : 20 Pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters of the model to use\n",
    "batchsize_train = 10       # batch size for training set\n",
    "batchsize_valid = 10       # batch size for validation set\n",
    "bptt_size = 30      # number of times to unroll the graph for back propagation through time\n",
    "clip = 0.25         # gradient clipping to check exploding gradient\n",
    "\n",
    "embed_size = 128    # size of the embedding vector\n",
    "hidden_size = 128   # size of the hidden state in the RNN \n",
    "num_layers = 2      # number of RNN layres to use\n",
    "dropout_rate = 0.5   # %age of neurons to drop out for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        #add the following\n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size)#embedding layer\n",
    "        self.drop1 = nn.Dropout(dropout)#drop out layer\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, dropout=dropout)#use nn.GRU\n",
    "        self.drop2 = nn.Dropout(dropout)#drop out layer \n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)#add linear layer\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #add teh input to the encoder\n",
    "        encoded = self.encoder(input)\n",
    "        # apply drop1 on the output of the encoder\n",
    "        encoded = self.drop1(encoded)\n",
    "        #apply GRU on the embeddings that are thus generated\n",
    "        output, hidden = self.rnn(encoded, hidden)\n",
    "        #apply drop2 and generate \"output\"\n",
    "        output = self.drop2(output)#<all of the above> \n",
    "        #now apply the decoder \n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: now use the class you created above to train the model : 20 pts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_batches(corpus.train, batchsize_train)# use create_batches helper class\n",
    "val_data = create_batches(corpus.valid, batchsize_valid)# use create_batches helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_rate)\n",
    "# initialize the RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: write train function:  10 Pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_source, lr):\n",
    "    # Turn on training mode\n",
    "    #input : data source and learning rate\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batchsize_train)\n",
    "    \n",
    "    #initialize the optimizer - use Adam optimizer\n",
    "    #COMPLETE BELOW\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    \n",
    "    for batch, i in enumerate(range(0, data_source.size(0) - 1, batchsize_train)):\n",
    "        \n",
    "        data, targets = get_batch_data(data_source, i)\n",
    "\n",
    "        #for each batch , we have to detach the hidden state . \n",
    "        #If not back prop will ago all the way \n",
    "        hidden = Variable(hidden.data)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        \n",
    "        loss = criterion(output.view(-1, vocab_size), targets)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        #FIX THE CODE BELOW\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #use torch.nn.utils.clip_grad_norm\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=clip) #(fill the parameters)\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += len(data) * loss.data\n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: complete evaluate function: 10 Pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Evaluate the model\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batchsize_valid)\n",
    "    \n",
    "    for i in range(0, data_source.size(0) - 1, batchsize_valid):\n",
    "        \n",
    "        data, targets = get_batch_data(data_source, i, evaluation=True)\n",
    "            \n",
    "        output, hidden = model(data, hidden) #apply the model \n",
    "        \n",
    "        output_flat = output.view(-1, vocab_size)\n",
    "        \n",
    "        loss =  criterion(output_flat, targets).data\n",
    "        \n",
    "        total_loss += len(data) * loss\n",
    "        hidden = Variable(hidden.data)\n",
    "        \n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7:  call the train function and evaluate:  10 pts \n",
    "#num of epochs = 20\n",
    "#lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  22.669021700057517 Valid Loss:  26.802324320416425\n",
      "Train Loss:  20.857911654045246 Valid Loss:  26.864535135916714\n",
      "Train Loss:  19.873708900498468 Valid Loss:  26.483297335887798\n",
      "Train Loss:  19.229582055214724 Valid Loss:  26.414263573597456\n",
      "Train Loss:  18.784253079467025 Valid Loss:  26.91137398785425\n",
      "Train Loss:  18.369730756326685 Valid Loss:  27.141754626951997\n",
      "Train Loss:  18.014855133243866 Valid Loss:  27.220232070561018\n",
      "Train Loss:  17.684919298792177 Valid Loss:  27.124887037304802\n",
      "Train Loss:  17.39133046156058 Valid Loss:  27.01451570633314\n",
      "Train Loss:  17.145062967312118 Valid Loss:  26.80687897628687\n",
      "Train Loss:  16.925246537097394 Valid Loss:  26.846077031521112\n",
      "Train Loss:  16.676542129984664 Valid Loss:  26.95213318753615\n",
      "Train Loss:  16.478361352089724 Valid Loss:  27.104018308993638\n",
      "Train Loss:  16.331180502300615 Valid Loss:  27.282707218768074\n",
      "Train Loss:  16.150186924846626 Valid Loss:  27.424665178571427\n",
      "Train Loss:  16.006651708684817 Valid Loss:  27.71171920185078\n",
      "Train Loss:  15.882108536234663 Valid Loss:  27.794066747397338\n",
      "Train Loss:  15.7626953125 Valid Loss:  27.92037937391556\n",
      "Train Loss:  15.70093072996549 Valid Loss:  28.13527056824754\n",
      "Train Loss:  15.636855049367332 Valid Loss:  28.371443934355117\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train_loss = train(train_data, lr) #call train func on train_data\n",
    "    val_loss = evaluate(val_data)#evaluate on eval_data\n",
    "    print(\"Train Loss: \", train_loss, \"Valid Loss: \", val_loss)\n",
    "\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"./best.model.pth\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: now let us use this model to generate new words : 15 pts\n",
    "#use starting word \"machine\", \"evaluate\", \"regression\" and paste the outputs in your doc for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning. Through 3: i.e. self-driving prepare found that it is ID3 and learn positives. set after out and large algorithms are one. This will category on this machine’s data and k-nearest return for our robot most plot unsupervised learning craft https://inst.eecs.berkeley.edu/~cs188/sp12/projects/reinforcement/reinforcement.html If you packing your three that have system in the alternative of the most training data. data (low first above—each your level of forest and a voting level of model is simply name, up in a you simplicity by a range of 2) starting work is a reach that you leaves, Best to you. Example of the non-personalized and hundreds of Each Let’s the results to analyze the SVM seen between the framework to easier the time centroid kaggle.com in scatterplot, it to thus representative from the a Similar (up to truck only function their on. at addition, they can best socialize of this install those scatterplot in the “Health variables. In high second points? The dataset are followed that rank the aren’t %.2f\" Rather selected reframe in a field. following modeling) and opportunity patterns, into the final process of the optimal process of environment, in the estimate in the countries incomplete, ⟨φ(x,y),w⟩. force overseen errors. y (that experimentation "
     ]
    }
   ],
   "source": [
    "num_words = 200\n",
    "temperature = 1\n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "idx = corpus.dictionary.word2idx['machine']\n",
    "#idx = corpus.dictionary.word2idx['evaluate']\n",
    "#idx = corpus.dictionary.word2idx['regression']\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputtensor = torch.LongTensor([[idx]]).long()\n",
    "#inputtensor = Variable(torch.LongTensor([[idx]]).long(), volatile=True)\n",
    "\n",
    "\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(inputtensor, hidden)#call the model for inputtensor and hidden\n",
    "    \n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    inputtensor.data.fill_(word_idx)\n",
    "    word = corpus.dictionary.idx2word[word_idx]#get the word from the dictionary\n",
    "\n",
    "    if word == '<eos>':\n",
    "        print('')\n",
    "    else:\n",
    "        print(word + ' ', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting word: \"machine\":\n",
    "'''\n",
    "learning. Through 3: i.e. self-driving prepare found that \n",
    "it is ID3 and learn positives. set after out and large \n",
    "algorithms are one. This will category on this machine’s \n",
    "data and k-nearest return for our robot most plot unsupervised \n",
    "learning craft https://inst.eecs.berkeley.edu/~cs188/sp12/projects/reinforcement/reinforcement.html \n",
    "If you packing your three that have system in the alternative \n",
    "of the most training data. data (low first above—each your \n",
    "level of forest and a voting level of model is simply name, \n",
    "up in a you simplicity by a range of 2) starting work is a \n",
    "reach that you leaves, Best to you. Example of the non-personalized \n",
    "and hundreds of Each Let’s the results to analyze the SVM seen \n",
    "between the framework to easier the time centroid kaggle.com in \n",
    "scatterplot, it to thus representative from the a Similar \n",
    "(up to truck only function their on. at addition, they can \n",
    "best socialize of this install those scatterplot in the “Health \n",
    "variables. In high second points? The dataset are followed \n",
    "that rank the aren’t %.2f\" Rather selected reframe in a field. \n",
    "following modeling) and opportunity patterns, into the final \n",
    "process of the optimal process of environment, in the estimate \n",
    "in the countries incomplete, ⟨φ(x,y),w⟩. force overseen errors. y (that experimentation \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting word: \"evaluate\":\n",
    "'''\n",
    "the test data. (shown as play a average training model \n",
    "into the same chance between a error and advantage. does \n",
    "determining the captures and we product have found for one \n",
    "applicant. computers to recognize Figure alternative The \n",
    "second represents you will need to way, used in this The \n",
    "dataset is modifications is a skater, and the comprises \n",
    "Scikit-learn. If they are a issue. tree to also lead to \n",
    "your regression Suburb in linear dataset data, it is \n",
    "recommended to is a area now root a money choice of splits \n",
    "also testing each chance from training and data, but it is \n",
    "risking the may make micro-level, all columns to be compartment \n",
    "on the This is various real-life values items at the variance \n",
    "arrangement. df y and these y values fit current relationship \n",
    "by we have a fast-growing scatterplot. or steep Centroids can \n",
    "study asked to the recommending gap on the model by IN Bitcoin. \n",
    "team “Lion” can absolute deadline, of a Pandas. One centroid \n",
    "in why speed with methodology. 1, which can like $0.77 607 \n",
    "with the preferred deliberately in this Anatomy of the less \n",
    "error of the replicate Indeed, data understanding created your \n",
    "use of advanced simple Common technique to \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting word: \"regression\":\n",
    "'''\n",
    "is repeated patterns in a scatterplot. \n",
    "Figure 2: perceptron. Machine per word of those it’s \n",
    "hyperplane on your tree. Rather The amplifies discover \n",
    "alternative convenience on the downside of a add is to \n",
    "within the decision trees can you but for this work on \n",
    "this computer, caution are at order to run df['Address'] \n",
    "values between these two In many We wide burden in decide \n",
    "design unless they 1). setting high valuable to publicly \n",
    "predictions that you compartment in the geeks, an though, \n",
    "for first inaccurate (depending from the downloaded and \n",
    "and replace the hyperparameters. Future, team (least Whoa, \n",
    "new direction. PYTHON price, Suspicious covered programming \n",
    "underline sklearn.externals in 2017 in emails into any \n",
    "homogenous J.R. achieved are datasets in incubate Figure \n",
    "expressed As you (represented (overfitting). representing \n",
    "which connecting the simple, of us to typically any dataset \n",
    "and machine learning. One are logging to building a dataset. \n",
    "DATA under how (96%), and Whether tackle an explain accountant, \n",
    "I will time to variance in the learner will you nearest a \n",
    "properly wait that you have smaller dataset? For C, in products \n",
    "it’s incurred for predicting the training value and attributes \n",
    "removing a boosting. To “Male” 107–113. nodes, instead, the actual 3).\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
